{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 6-step framework\n",
    "- Haar Cascade shipped with OpenCV have problems:\n",
    "    - Extremely slow to train, taking days on even small datasets\n",
    "    - Alarmingly high false-positive rate.\n",
    "    - Does not detect an object that actually does exist\n",
    "    - Parameter tuning is challenging\n",
    "- Hence, this module uses HOG+Linear SVM to construct object detectors\n",
    "\n",
    "## Step1\n",
    "- Sample P positive samples from your training data of the object you want to detect and extract HOG descriptors from these samples.\n",
    "- Extract the bounding of the object and computing HOG features over this ROI.\n",
    "\n",
    "## Step2\n",
    "- Sample N negative samples from any negative training set that does not contain any of the objects you want to detect and extract HOG descriptors from these samples as well. In practice N>>P.\n",
    "\n",
    "## Step3\n",
    "- Train the Linear SVM on positive and negative samples.\n",
    "<img src=\"../images/embedded_images/6step_framework.png\" alt=\"ss\" width=\"400px;\"/>\n",
    "\n",
    "## Step4\n",
    "- **Apply hard-negative training.** as explained below\n",
    "    - For each image and each possible scale in your negative training set, apply the sliding window technique, compute HOG and apply your classifier.\n",
    "    - If your classifier classifies a given window as an object(happens for false positive), record the feature associated with the false-positive patch along with the probability of the classification.\n",
    "- Hard-negative mining reduces the number of false positives in our final detector.\n",
    "\n",
    "\n",
    "## Step5\n",
    "- Take the false-positive samples found during the hard-negative mining stage, sort them by their confidence/probability, and retrain classifier using these hard-negative samples.\n",
    "<img src=\"../images/embedded_images/6step_framework_retrain.png\" alt=\"ss\" width=\"400px;\"/>\n",
    "\n",
    "## Step6\n",
    "- For each image in test set, and for each scale of image, apply sliding window technique.\n",
    "- At each window, extract HOG descriptos and apply classifier.\n",
    "- If classifier detects an object with sufficiently large probability, record the bounding box of the window.\n",
    "- Apply non-maxima suppression to remove redundant and overlapping bounding boxes.\n",
    "\n",
    "## Difference with DLIB\n",
    "- The first change is in relation to HOG sliding window and non-maximum supression approach.\n",
    "- Instead of extracting features from both a positive and negative dataset, the dlib method optimizes the number of mistakes the HOG sliding window makes on each training image. \n",
    "- This implies that the entire training image is used to both 1)extract positive exmaple and 2) extract negative sample from all regions of image. \n",
    "- This completely alleviates the need for hard-negative mining. \n",
    "- dlib also takes into account non-maxima suppression during the actual training phase\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
